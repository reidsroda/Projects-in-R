
---
title: "STAT340 HW07: Estimation II"
author: "TODO: Reid Sroda"
date: "October 2022"
output: html_document
---

***

TODO: If you worked with any other students on this homework, please list their names and NetIDs here.

***

## Instructions

Update the "author" and "date" fields in the header and
complete the exercises below.
Knit the document, and submit **both the HTML and RMD** files to Canvas.

__Due date:__ November 3, 2022 at 11:59pm.

---

This homework will review our discussion of estimation and confidence intervals from this week's lectures.

## 1) Comparing CLT- and simulation-based CIs

In lecture, we saw two different approaches to building confidence intervals.

* One, hopefully familiar to you from previous introductory courses, was based on the CLT, namely the fact that the sample mean is (approximately) normal about the population mean after appropriate rescaling (i.e., dividing by the standard deviation).
* The other, probably new to you, was simulation-based.
We estimate our parameter(s) of interest, then generate new data based on those (estimated) parameters.
We can then use that data to estimate the parameter(s) *again*, and the behavior of those "fake data" estimates can tell us about the behavior of our estimator with respect to the true parameter(s).

One thing we didn't discuss in lecture is how these two methods compare (except to say that... it depends).

This problem is fairly open-ended. Your job is to set up an experiment comparing these two different confidence interval methods.

### Part a: The data generating model

First things first, we need to generate data.
Define a function `generate_data`, that takes two arguments, `lambda` and `nsamp`, where `lambda` is the parameter of a Poisson and `nsamp` is a positive integer specifying a number of samples.
Your function should return a vector of length `nsamp`, whose entries are drawn iid from Poisson with parameter given by `lambda`.
`lambda` should default to 1.

```{r}
generate_data <- function( nsamp, lambda=1 ) {
  data = rep(0, nsamp)
  for(i in 1:nsamp){
    data[i] = rpois(1, lambda)
  }
  return(data)
}
generate_data(20, 1)
```

### Part b: fitting the data

Our simulation-based method requires that we have an estimate of the mean, and our CLT-based method requires that we have estimates of the mean and variance.
Write functions `estimate_pois_mean` and `estimate_pois_var` to estimate both the mean and variance of the Poisson given the output of `generate_data`.

__Hint:__ the Poisson has the convenient property that the expectation and variance are both equal to $\lambda$, so these can be the same function-- both can just return the sample mean, if you want, but you can also use the sample variance. Your choice!

```{r}
estimate_pois_mean <- function( data ) {
  # TODO: code goes here. Estimate lambda as the sample mean.
  return(mean(data))
}

estimate_pois_var <- function( data ) {
  # TODO: code goes here. Estimate the variance.

  # One option is to be lazy and use the fact that mean=var=lambda in Poisson
  # Like this: return( estimate_pois_mean( data ) );
}
estimate_pois_mean(generate_data(20,1))
```

### Part c: CLT-based confidence intervals

Implement a function `CLT_CI` that takes arguments `data` (the vector of data generated by `generate_data`) and `alpha` (a numeric between 0 and 1, i.e, a probability, which should default to $0.05$), and returns a confidence interval (i.e., a vector of length 2 whose first entry is the left end of the interval and whose second entry is the right end of the interval).

This confidence interval should be based on our CLT-based approximation. That is, you should use the fact that
$$
  \frac{ \bar{X} - \lambda }{ \sqrt{ (\operatorname{Var X_1})/n }}
$$
is approximately normal.

Use your `estimate_pois_var` function to estimate the variance term in the denominator (ignore the fact that this is equal to $\lambda$), use `estimate_pois_mean` to get $\bar{X}$ and follow the basic outline from lecture to create a $(1-\alpha)*100$-percent *two-sided* confidence interval (if "two-sided" is not familiar to you, no worries-- just follow the recipe from lecture and you'll be fine!).

```{r}
CLT_CI <- function( data, alpha=0.05) {
  # TODO: code here.
  
  # Step 1. use data to get Xbar and the variance estimate.
  Xbar = estimate_pois_mean(data)
  
  # Step 2. Use the estimated variance and choice of alpha to construct
  # a two-sided confidence interval for lambda
  # (sorry, Z-scores are going to show up here, but only briefly!)
  var = estimate_pois_mean(data)
  n = length(data)
  z = qnorm(1-alpha)
  
  # Return the CI. Be careful-- depending on how you got your CI, it
  # might not be a simple vector (e.g., it might have a header).
  # Feel free to use the lecture code for reference.
  conf = c( Xbar - z*sqrt( var/n ), Xbar + z*sqrt( var/n ))
  return(conf)
}
CLT_CI(generate_data(20,1), alpha=.05)
```


### Part d: simulation-based confidence intervals

Implement a function `simulation_CI` that takes arguments `data` (the vector of data generated by `generate_data`) and `alpha` (a numeric between o and 1, i.e, a probability, which should default to $0.05$), and returns a confidence interval (i.e., a vector of length 2 whose first entry is the left end of the interval and whose second entry is the right end of the interval).

Your function should construct the confidence interval according to the simulation-based approach discussed in class.
You are free to adapt the code from lecture in your code.
You may pick the number of Monte Carlo iterates (i.e., replicates) as you wish, but we would recommend setting it to be at least a few hundred.

```{r}
simulation_CI <- function( data, alpha=0.05) {
  n = length(data)
  Nrep = 1000
  replicates = rep(0,Nrep)
  # TODO: code goes here.
  for ( i in 1:Nrep) {
    fake_data <- rpois(n=n, lambda=1);
    replicates[i] <- mean( fake_data );
  }
  
  # Return a vector c(L, U) with L the left end and U the right ("upper") end of the CI. 
  CI <- unname(quantile( replicates, probs=c(alpha/2, 1 - (alpha/2))) );
  return(CI)
}
simulation_CI(generate_data(20,1), alpha=.05)


```

### Part e: Comparing Confidence Intervals

So, now you've implemented two different confidence intervals.
Let's compare them.

First of all, we need to be able to check if our CI "caught" the true parameter or not.

Write a function `contained` that takes a CI (i.e., a two-vector with first entry smaller than the second) and a number (the true value of the parameter) that returns TRUE if the true value of the parameter is inside the interval and FALSE otherwise.

```{r}
contained <- function( myCI, trueparam ) {
  # TODO: code goes here.
  # Hint: the estimation lecture notes include code to do this.
  return((myCI[1] < trueparam) & (trueparam < myCI[2]))
}
contained(simulation_CI(generate_data(20,1), alpha=.05), 1.7)
```

What does it mean for one confidence interval to be better than another?
We define the *coverage* of a confidence interval to be the probability that the true parameter lies in the interval.

Ideally, a $1-\alpha$ confidence interval has coverage $1-\alpha$. Simple!

The problem arises from approximation-- our CLT- and simulation-based CIs are based on approximations, and so their coverage will not be exactly $1-\alpha$.

Implement a function called `estimate_coverage`, whose arguments are given in the code block below, and which returns an estimate of the coverage (i.e., a number between 0 and 1).

* `CI_fn` is a *function* (in our code, this will be either `CLT_CI` or `simulation_CI`). Yes, functions can be arguments to other functions in R! Note that this argument will just be a function. *Not a string.*
* `NMC` is a positive integer, the number of replicates to run.
* `nsamp` is a positive integer, the number of samples.
* `lambdatrue` is a positive real, the parameter of the Poisson that we will use to generate our data. It should default to 1.
* `alpha` is a numeric between 0 and 1. `1-alpha` will be the (target) level of our CI.

```{r}
estimate_coverage <- function( CI_fn, NMC, nsamp, lambdatrue=1, alpha=0.05 ) {
  coverages <- 0; # Count how often the CI "catches" lambdatrue
  
  for (i in 1:NMC ) {
    # Generate data: nsamp draws from Poisson( lambdatrue )
    data <- rpois(n=nsamp, lambda=lambdatrue);

    # Construct a confidence interval from it using the given CI function
    myCI = CI_fn(data, alpha)

    # if lambdatrue is in the CI, increment coverages.
    if(contained(myCI, lambdatrue) == TRUE){
      coverages = coverages + 1
    }
    else{
      coverages = coverages
    }
  }
  
  return( coverages/NMC )
}
estimate_coverage(CLT_CI, 100, 20, lambdatrue = 1, alpha = .05)
```


### Part f: exploring

Wow, that was a lot of coding! Here comes the payoff!

Use the code above to estimate the coverage of your two CI methods.
Try changing the different arguments-- $\lambda$, $\alpha$, `nsamp`, etc.
Coverages of the two methods *should* both be close to $1-\alpha$, whatever you chose $\alpha$ to be, but you will likely find that they tend to follow patterns, consistently either over- or under-shooting the target.

Take some time to explore these patterns. What do you see?
There are no right or wrong answers, here.
We are just asking you to see how these two different CI methods behave in different situations.

An important point that we alluded to above is that in the Poisson, the mean and variance are both just $\lambda$.
This is actually part of why you may have noticed some weird behavior in your experiments above.
Unfortunately, a thorough explanation of that weird behavior will have to wait for your later theory courses.

```{r}
estimate_coverage(CLT_CI, 100, 20, lambdatrue = 1, alpha = .05)
estimate_coverage(simulation_CI, 100, 20, lambdatrue = 1, alpha = .05)

```


***

Our CLT_CI function seems to be roughly around the 1-alpha level that we predicted, while the simulation_CI function seems to either get all of it correct or none of it correct which leads me to believe our CLT_CI function is more accurate than the simulation_CI. As you increase the alpha level the CLT_CI is far more accurate than the simulation_CI.

***

## 2) The infamous mule kick data

The file `mule_kicks.csv`, available for download (here)[https://kdlevin-uwstat.github.io/STAT340-Fall2021/hw/03/mule_kicks.csv], contains a simplified version of a very famous data set.
The data consists of the number of soldiers killed by being kicked by mules or horses each year in a number of different companies in the Prussian army near the end of the 19th century.

This may seem at first to be a very silly thing to collect data about, but it is a very interesting thing to look at if you are interested in rare events.
Deaths by horse kick were rare events that occurred independently of one another, and thus it is precisely the kind of process that we might expect to obey a Poisson distribution.

Download the data and read it into R by running
```{r}
download.file('https://kdlevin-uwstat.github.io/STAT340-Fall2021/hw/03/mule_kicks.csv', destfile='mule_kicks.csv');
mule_kicks <- read.csv('mule_kicks.csv', header=TRUE);

head(mule_kicks);
```

`mule_kicks` contains a single column, called `deaths`.
Each entry is the number of soldiers killed in one corps of the Prussian army in one year.
There are 14 corps in the data set, studied over 20 years, for a total of 280 death counts.

### Part a: estimating the Poisson rate

Assuming that the mule kicks data follows a Poisson distribution, produce a point estimate for the rate parameter $\lambda$.
There are no strictly right or wrong answers, here, though there are certainly better or worse ones.

```{r}
#TODO: estimate the rate parameter.
library(tidyverse)
library(dplyr)
data2 = mule_kicks %>%  pull(deaths)

lambdahat <- mean(data2); # TODO: write code; store your estimate in lambdahat.
lambdahat
```

### Part b: constructing a CI

Using everything you know (Monte Carlo, CLT, etc.), construct a confidence interval for the rate parameter $\lambda$.
Explain in reasonable detail what you are doing and why you are constructing the confidence interval in this way (a few sentences is fine!).

```{r}
CLT_CI(data2)
```

***

I chose to use the central limit theorem confidence interval because it was a little more accurate than the simulation intervals, since the central limit theorem still applies to poission distributions this confidence interval is valid. The results also makes sense.

***

### Part c: assessing a model

Here's a slightly more open-ended question.
We *assumed* that the data followed a Poisson distribution.
This may or may not be a reasonable assumption.
Use any and all tools that you know to assess (either with code or simply in words) how reasonable or unreasonable this assumption is.

Once again, there are no strictly right or wrong answers here.
Explain and defend your decisions and thought processes in a reasonable way and you will receive full credit.

***

```{r}
plot(data2, dpois(data2, lambdahat), type = 'h')
```
With a lambdahat value of less than 1 the graph should be extremely left heavy, which is exactly what is displayed in the graph here.

***

